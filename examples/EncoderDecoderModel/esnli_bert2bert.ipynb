{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess esnli data from .csv files to batches of tensors ready to feed into models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./../../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataProcessor, InputExample\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsnliProcessor(DataProcessor):\n",
    "\n",
    "    def get_train_examples(self, data_path):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        examples = []\n",
    "        with open(data_path, newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for (i, line) in enumerate(reader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                guid = \"%s-%s\" % (\"train\", i)\n",
    "                label = line[1]\n",
    "                premise = line[2]\n",
    "                hypothesis = line[3]\n",
    "                text_a = premise + \" [SEP] \" + hypothesis # p + [SEP] + h\n",
    "                text_b = line[4] # expl\n",
    "                assert isinstance(text_a, str) and isinstance(text_b, str) and isinstance(label, str)\n",
    "                examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "    \n",
    "    def get_dev_examples(self, data_path):\n",
    "        examples = []\n",
    "        with open(data_path, newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for (i, line) in enumerate(reader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                guid = \"%s-%s\" % (\"dev\", i)\n",
    "                label = line[1]\n",
    "                premise = line[2]\n",
    "                hypothesis = line[3]\n",
    "                text_a = premise + \" [SEP] \" + hypothesis # p + [SEP] + h\n",
    "                text_b = line[4] # expl\n",
    "                assert isinstance(text_a, str) and isinstance(text_b, str) and isinstance(label, str)\n",
    "                examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = EsnliProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using dev 100 examples here because it is a smaller dataset so faster to test my code\n",
    "dev_examples = processor.get_dev_examples('./esnli_dev.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dev_examples))\n",
    "print(dev_examples[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Examples to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as logger\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention mask: avoid performing attention on padding token indices\n",
    "# padding and truncation to max length\n",
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esnli_examples_to_features(examples, max_seq_len, tokenizer, cls_token='[CLS]', sep_token='[SEP]', \n",
    "                               pad_token=0, mask_padding_with_zero=True):\n",
    "    \"\"\"\n",
    "        Does not support token_type_id, because the EncoderDecoderModel does not. Therefore, the premise\n",
    "        and hypothesis is separated by a [SEP], but no token_type_id is there to tell this difference.\n",
    "    \"\"\"\n",
    "    \n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 1000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        input_ids, input_mask = text_to_input_ids(example.text_a, max_seq_len, tokenizer)\n",
    "        decoder_input_ids, dummy = text_to_input_ids(example.text_b, max_seq_len, tokenizer)\n",
    "    \n",
    "        assert len(input_ids) == max_seq_len\n",
    "        assert len(input_mask) == max_seq_len\n",
    "        assert len(decoder_input_ids) == max_seq_len\n",
    "\n",
    "        features.append(EsnliInputFeatures(input_ids=input_ids,\n",
    "                                          attention_mask=input_mask,\n",
    "                                          decoder_input_ids=decoder_input_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_input_ids(text, max_seq_len, tokenizer, cls_token='[CLS]', sep_token='[SEP]', \n",
    "                      pad_token=0, mask_padding_with_zero=True):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # truncate to max_length - 2 if needed, \n",
    "    # the -2 accounts for cls_token and sep_token that are going to be added.\n",
    "    tokens = tokens[:max_seq_len-2]\n",
    "    \n",
    "    tokens = [cls_token] + tokens + [sep_token]\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_len - len(input_ids)\n",
    "    input_ids = input_ids + ([pad_token] * padding_length)\n",
    "    input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "    \n",
    "    return input_ids, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsnliInputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, attention_mask, decoder_input_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.decoder_input_ids = decoder_input_ids\n",
    "        self.labels = decoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_features = esnli_examples_to_features(dev_examples, max_seq_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dev_features[13].input_ids)\n",
    "print(dev_features[13].attention_mask)\n",
    "print(dev_features[13].decoder_input_ids)\n",
    "print(dev_features[13].labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features to Tensor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensors \n",
    "dev_all_input_ids = torch.tensor([f.input_ids for f in dev_features], dtype=torch.long)\n",
    "dev_all_attention_mask = torch.tensor([f.attention_mask for f in dev_features], dtype=torch.long)\n",
    "dev_all_decoder_input_ids = torch.tensor([f.decoder_input_ids for f in dev_features], dtype=torch.long)\n",
    "dev_all_labels = torch.tensor([f.labels for f in dev_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = TensorDataset(dev_all_input_ids, dev_all_attention_mask, dev_all_decoder_input_ids, dev_all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume we use the default_data_collator from HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize Bert2Bert\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dev_features,         # training dataset\n",
    "    eval_dataset=dev_features            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call trainer.train() to train and trainer.evaluate(). \n",
    "# The first argument returned from forward must be the loss which you wish to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosa",
   "language": "python",
   "name": "rosa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
