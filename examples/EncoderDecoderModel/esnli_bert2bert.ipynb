{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess esnli data from .csv files to batches of tensors ready to feed into models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./../../src')\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '/data/rosa/data/esnli/esnli_train.csv'\n",
    "dev_data_path = './sanity-checks/esnli_dev_100.csv'\n",
    "cached_train_features_file = './cache/cached_train_esnli'\n",
    "save_trained_model_dir = \"./esnli_train_trained_model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataProcessor, InputExample\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsnliProcessor(DataProcessor):\n",
    "\n",
    "    def get_train_examples(self, data_path):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        examples = []\n",
    "        with open(data_path, newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for (i, line) in enumerate(reader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                guid = \"%s-%s\" % (\"train\", i)\n",
    "                label = line[1]\n",
    "                premise = line[2]\n",
    "                hypothesis = line[3]\n",
    "                text_a = premise + \" [SEP] \" + hypothesis # p + [SEP] + h\n",
    "                text_b = line[4] # expl\n",
    "                assert isinstance(text_a, str) and isinstance(text_b, str) and isinstance(label, str)\n",
    "                examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "    \n",
    "    def get_dev_examples(self, data_path):\n",
    "        examples = []\n",
    "        with open(data_path, newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for (i, line) in enumerate(reader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                guid = \"%s-%s\" % (\"dev\", i)\n",
    "                label = line[1]\n",
    "                premise = line[2]\n",
    "                hypothesis = line[3]\n",
    "                text_a = premise + \" [SEP] \" + hypothesis # p + [SEP] + h\n",
    "                text_b = line[4] # expl 1\n",
    "                text_c = line[9] # expl 2\n",
    "                text_d = line[14] # expl 3\n",
    "                assert isinstance(text_a, str) and isinstance(text_b, str) and isinstance(label, str) \\\n",
    "                and isinstance(text_c, str) and isinstance(text_d, str)\n",
    "                examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, \\\n",
    "                                             label=label, text_c=text_c, text_d=text_d))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = EsnliProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using a smaller dataset so faster to test my code\n",
    "train_examples = processor.get_train_examples(train_data_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_examples = processor.get_dev_examples(dev_data_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Examples to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as logger\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention mask: avoid performing attention on padding token indices\n",
    "# padding and truncation to max length\n",
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esnli_examples_to_features(examples, max_seq_len, tokenizer, cls_token='[CLS]', sep_token='[SEP]', \n",
    "                               pad_token=0, mask_padding_with_zero=True):\n",
    "    \"\"\"\n",
    "        Does not support token_type_id, because the EncoderDecoderModel does not. Therefore, the premise\n",
    "        and hypothesis is separated by a [SEP], but no token_type_id is there to tell this difference.\n",
    "    \"\"\"\n",
    "    \n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 1000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        input_ids, input_mask = text_to_input_ids(example.text_a, max_seq_len, tokenizer)\n",
    "        decoder_input_ids, dummy = text_to_input_ids(example.text_b, max_seq_len, tokenizer)\n",
    "        assert len(input_ids) == max_seq_len\n",
    "        assert len(input_mask) == max_seq_len\n",
    "        assert len(decoder_input_ids) == max_seq_len\n",
    "        \n",
    "        expl2_ids = None\n",
    "        expl3_ids = None\n",
    "        if example.text_c != None and example.text_d != None:\n",
    "            expl2_ids, dummy = text_to_input_ids(example.text_c, max_seq_len, tokenizer)\n",
    "            expl3_ids, dummy = text_to_input_ids(example.text_d, max_seq_len, tokenizer)\n",
    "            assert len(expl2_ids) == max_seq_len\n",
    "            assert len(expl3_ids) == max_seq_len\n",
    "        \n",
    "\n",
    "        features.append(EsnliInputFeatures(input_ids=input_ids,\n",
    "                                          attention_mask=input_mask,\n",
    "                                          decoder_input_ids=decoder_input_ids,\n",
    "                                          expl2_ids=expl2_ids,\n",
    "                                          expl3_ids=expl3_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_input_ids(text, max_seq_len, tokenizer, cls_token='[CLS]', sep_token='[SEP]', \n",
    "                      pad_token=0, mask_padding_with_zero=True):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # truncate to max_length - 2 if needed, \n",
    "    # the -2 accounts for cls_token and sep_token that are going to be added.\n",
    "    tokens = tokens[:max_seq_len-2]\n",
    "    \n",
    "    tokens = [cls_token] + tokens + [sep_token]\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_len - len(input_ids)\n",
    "    input_ids = input_ids + ([pad_token] * padding_length)\n",
    "    input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "    \n",
    "    return input_ids, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsnliInputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, attention_mask, decoder_input_ids, expl2_ids, expl3_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.decoder_input_ids = decoder_input_ids\n",
    "        self.labels = decoder_input_ids #expl1\n",
    "        self.expl2 = expl2_ids #expl2\n",
    "        self.expl3 = expl3_ids #expl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache training dataset features\n",
    "if os.path.exists(cached_train_features_file):\n",
    "    logger.info(\"Loading features from cached file %s\", cached_train_features_file)\n",
    "    train_features = torch.load(cached_train_features_file)\n",
    "else:\n",
    "    train_features = esnli_examples_to_features(train_examples, max_seq_len, tokenizer)\n",
    "    logger.info(\"Saving training features into cached file %s\", cached_train_features_file)\n",
    "    torch.save(train_features, cached_train_features_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_features = esnli_examples_to_features(dev_examples, max_seq_len, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize Bert2Bert\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoint-train-results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./train-logs',            # directory for storing logs\n",
    "    do_train=True,\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    overwrite_output_dir=True,\n",
    "    warmup_steps=1000,                # number of warmup steps for learning rate scheduler\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_features,         # training dataset\n",
    "    eval_dataset=train_features            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call trainer.train() to train.\n",
    "# The first argument returned from forward must be the loss which you wish to optimize.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = save_trained_model_dir\n",
    "cuda_id = \"1\" # since there's something running on 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = EncoderDecoderModel.from_pretrained(output_dir)\n",
    "device = torch.device(\"cuda:\"+cuda_id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_args = TrainingArguments(\n",
    "    output_dir='./checkpoint-eval-results',          # output directory\n",
    "    per_device_eval_batch_size=2,   # batch size for evaluation\n",
    "    logging_dir='./eval-logs',            # directory for storing logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=eval_args,                  # eval arguments, defined above\n",
    "    eval_dataset=dev_features,            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluator.eval_esnli_write_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute bleu scores based on csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu_score(pred_gold_csv):\n",
    "    \"\"\"\n",
    "    Compute bleu score based on the predicted explanations and the gold explanations in the csv file.\n",
    "\n",
    "    Input: csv file\n",
    "    Output: bleu score\n",
    "    \"\"\"\n",
    "    bleu_scores = []\n",
    "    with open(pred_gold_csv, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for (i, line) in enumerate(reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if i % 50 == 0:\n",
    "                print(i)\n",
    "                print('pred_expl: ', tokenizer.decode(pred_expl))\n",
    "                print('gold_expl_1: ', tokenizer.decode(gold_expl_1))\n",
    "                print('gold_expl_2: ', tokenizer.decode(gold_expl_2))\n",
    "                print('gold_expl_3: ', tokenizer.decode(gold_expl_3))\n",
    "            pred_expl = eval(line[0])\n",
    "            gold_expl_1 = eval(line[1])\n",
    "            gold_expl_2 = eval(line[2])\n",
    "            gold_expl_3 = eval(line[3])\n",
    "            # process the explanations before passing to compute bleu scores: \n",
    "            # get rid of the CLS, SEP, can PAD tokens - tokens with id 101, 102, and 0\n",
    "            pred_expl = remove_special_tokens(pred_expl)\n",
    "            gold_expl_1 = remove_special_tokens(gold_expl_1)\n",
    "            gold_expl_2 = remove_special_tokens(gold_expl_2)\n",
    "            gold_expl_3 = remove_special_tokens(gold_expl_3)\n",
    "            \n",
    "            bleu_scores.append(corpus_bleu([[gold_expl_1, gold_expl_2, gold_expl_3]], [pred_expl]))\n",
    "    return mean(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_tokens(token_list):    \n",
    "    sep_index = token_list.index(102) if 102 in token_list else -1\n",
    "    cls_index = token_list.index(101) if 101 in token_list else -1\n",
    "    \n",
    "    result = []\n",
    "    # remove [sep] and [pad]\n",
    "    if sep_index == -1:\n",
    "        result = token_list\n",
    "    else:\n",
    "        result = token_list[:sep_index]\n",
    "    \n",
    "    # remove [cls]\n",
    "    if cls_index == -1:\n",
    "        return result\n",
    "    else:\n",
    "        return result[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bleu_score('./dafsdhfasjg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
